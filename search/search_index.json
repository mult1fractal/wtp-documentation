{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CaSe-Group What the Phage: A scalable workflow for the identification and analysis of phage sequences M. Marquet, M. H\u00f6lzer, M. W. Pletz, A. Viehweger, O. Makarewicz, R. Ehricht, C. Brandt doi: https://doi.org/10.1101/2020.07.24.219899 System Requirements Components minimum recomended OS Linux like Linux like Cores 4 8 Memory 4 GB RAM 8 GB RAM Storage 75 GB available space 128-256 GB available space Why so much space? -.- What the Phage Phages are among the most abundant and diverse biological entities on earth. Identification from sequence data is a crucial first step to understand their impact on the environment. A variety of bacteriophage identification tools have been developed over the years. They differ in algorithmic approach, results and ease of use. We, therefore, developed \u201cWhat the Phage\u201d (WtP), an easy-to-use and parallel multitool approach for phage identification combined with an annotation and classification downstream strategy, thus, supporting the user\u2019s decision-making process when the phage identification tools are not in agreement to each other. WtP is reproducible and scales to thousands of datasets through the use of a workflow manager (Nextflow). Example result output Example result report Under the hood Figure 3: This plot shows a simplified Flowchart of WtP for better understanding of what's going on behind the curtain Included tools Identification Toolname/Gitlink Reference VirFinder VirFinder: R package for identifying viral sequences from metagenomic data using sequence signatures PPR-Meta PPR-Meta: a tool for identifying phages and plasmids from metagenomic fragments using deep learning VirSorter VirSorter: mining viral signal from microbial genomic data MetaPhinder MetaPhinder\u2014Identifying Bacteriophage Sequences in Metagenomic Data Sets DeepVirFinder Identifying viruses from metagenomic data by deep learning Sourmash sourmash: a library for MinHash sketching of DNA VIBRANT Automated recovery, annotation and curation of microbial viruses, and evaluation of virome function from genomic sequences VirNet Deep attention model for viral reads identification Phigaro Phigaro: high throughput prophage sequence annotation Virsorter2 VirSorter2: a multi-classifier, expert-guided approach to detect diverse DNA and RNA viruses Seeker Seeker: alignment-free identification of bacteriophage genomes by deep learning Annotation & classification Toolname/Git Reference prodigal Prodigal: prokaryotic gene recognition and translation initiation site identification hmmer nhmmer: DNA homology search with profile HMMs chromomap CheckV CheckV: assessing the quality of metagenome-assembled viral genomes Other tools Toolname/Git Reference samtools The Sequence Alignment/Map format and SAMtools seqkit SeqKit: A Cross-Platform and Ultrafast Toolkit for FASTA/Q File Manipulation UpSetR UpSetR: an R package for the visualization of intersecting sets and their properties .","title":"Home"},{"location":"#system-requirements","text":"Components minimum recomended OS Linux like Linux like Cores 4 8 Memory 4 GB RAM 8 GB RAM Storage 75 GB available space 128-256 GB available space Why so much space? -.-","title":"System Requirements"},{"location":"#what-the-phage","text":"Phages are among the most abundant and diverse biological entities on earth. Identification from sequence data is a crucial first step to understand their impact on the environment. A variety of bacteriophage identification tools have been developed over the years. They differ in algorithmic approach, results and ease of use. We, therefore, developed \u201cWhat the Phage\u201d (WtP), an easy-to-use and parallel multitool approach for phage identification combined with an annotation and classification downstream strategy, thus, supporting the user\u2019s decision-making process when the phage identification tools are not in agreement to each other. WtP is reproducible and scales to thousands of datasets through the use of a workflow manager (Nextflow).","title":"What the Phage"},{"location":"#example-result-output","text":"Example result report","title":"Example result output"},{"location":"#under-the-hood","text":"Figure 3: This plot shows a simplified Flowchart of WtP for better understanding of what's going on behind the curtain","title":"Under the hood"},{"location":"#included-tools","text":"","title":"Included tools"},{"location":"#identification","text":"Toolname/Gitlink Reference VirFinder VirFinder: R package for identifying viral sequences from metagenomic data using sequence signatures PPR-Meta PPR-Meta: a tool for identifying phages and plasmids from metagenomic fragments using deep learning VirSorter VirSorter: mining viral signal from microbial genomic data MetaPhinder MetaPhinder\u2014Identifying Bacteriophage Sequences in Metagenomic Data Sets DeepVirFinder Identifying viruses from metagenomic data by deep learning Sourmash sourmash: a library for MinHash sketching of DNA VIBRANT Automated recovery, annotation and curation of microbial viruses, and evaluation of virome function from genomic sequences VirNet Deep attention model for viral reads identification Phigaro Phigaro: high throughput prophage sequence annotation Virsorter2 VirSorter2: a multi-classifier, expert-guided approach to detect diverse DNA and RNA viruses Seeker Seeker: alignment-free identification of bacteriophage genomes by deep learning","title":"Identification"},{"location":"#annotation-classification","text":"Toolname/Git Reference prodigal Prodigal: prokaryotic gene recognition and translation initiation site identification hmmer nhmmer: DNA homology search with profile HMMs chromomap CheckV CheckV: assessing the quality of metagenome-assembled viral genomes","title":"Annotation &amp; classification"},{"location":"#other-tools","text":"Toolname/Git Reference samtools The Sequence Alignment/Map format and SAMtools seqkit SeqKit: A Cross-Platform and Ultrafast Toolkit for FASTA/Q File Manipulation UpSetR UpSetR: an R package for the visualization of intersecting sets and their properties .","title":"Other tools"},{"location":"troubleshooting/","text":"FAQ In this section I have some troubleshooting advice with problems I faced when I started with bioinformatics and testing with WtP Input fasta-file size We encountered some WtP long run issues if the file size exceeds a certain size ( see this issue ), and WtP seems to be stuck. (maybe very long run times of some tools) Our fasta.gz testfiles are below below 40 MB in file size If your fasta files exceed 80 - 100 MB, please split them into smaller chunks Problems with storage while running WtP WtP produces temporary data. Depending on your input file, this temporary data can take up a lot of GB of storage space after several WtP runs By default, all temporary data files are stored in /tmp/nextflow-phage-$USER . i.e. When you restart, the temporary data files will be removed. For users who run WtP on a cluster and can't restart it, we have the --work-dir flag. This makes it possible to change the storage location for the temporary workflow files. With the flag --work-dir work , a folder with the name work will be created in your current working dir. All of WtP temporary workflow files will be stored in this directory. With sudo rm -r current_working_dir/work* they can become demanding. --work-dir /path/to/dir # defines the path where nextflow writes temporary files, default: '/tmp/nextflow-phage-$USER' Docker-files needed and their size # check size of dockerimages that are needed docker images --format \"table {{.Repository}}\\t{{.Tag}}\\t{{.Size}}\" REPOSITORY TAG SIZE multifractal/template_pandas v3.8.p 817MB nanozoo/rmarkdown 2.10--a3f4088 3.72GB papanikos/marvel 0.2-29b3c73 6.42GB papanikos/virsorter-2 2.2.1--fa935f8 1.19GB multifractal/seeker 0.1 1.66GB multifractal/phigaro 0.5.2 2.6GB nanozoo/seqkit 0.13.2--cd66104 469MB multifractal/virnet-hack 0.1 1.62GB nanozoo/emboss 6.6.0--418c521 1.07GB multifractal/ppr-meta 0.3.1 5.3GB multifractal/virfinder 0.2 3.91GB multifractal/vibrant 0.5 1.42GB nanozoo/sourmash 3.4.1--16a8db7 788MB nanozoo/hmmer 3.3--3db9dd1 484MB nanozoo/checkv 0.6.0--e97f45e 1.72GB nanozoo/altair 4.1.0--086b80e 1.02GB nanozoo/samtools 1.9--76b9270 487MB multifractal/virsorter 0.1.2 2.84GB nanozoo/r_fungi 0.1--097b1bb 3.11GB nanozoo/template 3.8--ccd0653 681MB nanozoo/basics 1.0--962b907 79.1MB nanozoo/upsetr 1.4.0--0ea25b3 3.21GB nanozoo/r_ggplot2 0.1--6405f6d 3.26GB multifractal/metaphinder 0.1 767MB multifractal/deepvirfinder 0.1 2.37GB nanozoo/prodigal 2.6.3--2769024 531MB Installing WTP in a centralized way for multi-users Run WtP on a cluster environment: Let the Users run WtP from their individual accounts (e.g., via ssh connection) create special locations (shared locations) where to store the singularity images, docker images, the databases and the cache let each run of WtP (from a user) use the shared locations but the usage provided forces the users to specify params --workdir, --databases and --cache...... the shared locations should be transparent to the users .. Quick Solution: \"install\" WtP via git clone --branch v1.2.0 https://github.com/replikation/What_the_Phage.git then change the nextflow config and let the user use this \"git\" (the version would then be fixed to the git clone) e.g.: ./phage.nf \\ --fasta /path/to/file.fa \\ # provide a fasta-file as input --cores 8 \\ # number of cores you want to use -profile local,docker # choose the environment:local and docker Error ignored: some tools can fail during the identification process e.g. if a tool can't process a fasta-inputfile (sometimes MARVEL with really large multi fasta files) then you will get an error-message like this : [34/31d18f] NOTE: Process `identify_fasta_MSF:upsetr_plot (1)` terminated with an error exit status (1) -- Error is ignored even if an error occures WtP continues its work, but won't include the results of the failed process Chromomap issues: terminated with an error exit status (1) In the Annotation process of WtP, wird eine Genkarte mit den annotierten/gefundenen Genen erstellt Bei der erstellung der Genkarte kann es manchmal zu folgendem Error-code kommen: [4e/57d82d] NOTE: Process 'phage_annotation_MSF:chromomap (1)' terminated with an error exit status (1) -- Execution is retried (1) The workflow works as intended. We included a few \"failsafe\" processes for chromomap, so if one plotting approach fails it retries another approach to render it, thus you get the fail but the retry will work. In the end you will get the result Singularity image problems Sometimes the singularity runs fail because some singularity images are failing in their build process: you can retry the the execution command or you can run the Pre-download for Offline-mode and start WtP with the 'preloaded' images Download Databases manually usually there is no problem with database download. in case there is a problem you can download them here","title":"Troubleshooting"},{"location":"troubleshooting/#faq","text":"In this section I have some troubleshooting advice with problems I faced when I started with bioinformatics and testing with WtP","title":"FAQ"},{"location":"troubleshooting/#input-fasta-file-size","text":"We encountered some WtP long run issues if the file size exceeds a certain size ( see this issue ), and WtP seems to be stuck. (maybe very long run times of some tools) Our fasta.gz testfiles are below below 40 MB in file size If your fasta files exceed 80 - 100 MB, please split them into smaller chunks","title":"Input fasta-file size"},{"location":"troubleshooting/#problems-with-storage-while-running-wtp","text":"WtP produces temporary data. Depending on your input file, this temporary data can take up a lot of GB of storage space after several WtP runs By default, all temporary data files are stored in /tmp/nextflow-phage-$USER . i.e. When you restart, the temporary data files will be removed. For users who run WtP on a cluster and can't restart it, we have the --work-dir flag. This makes it possible to change the storage location for the temporary workflow files. With the flag --work-dir work , a folder with the name work will be created in your current working dir. All of WtP temporary workflow files will be stored in this directory. With sudo rm -r current_working_dir/work* they can become demanding. --work-dir /path/to/dir # defines the path where nextflow writes temporary files, default: '/tmp/nextflow-phage-$USER'","title":"Problems with storage while running WtP"},{"location":"troubleshooting/#docker-files-needed-and-their-size","text":"# check size of dockerimages that are needed docker images --format \"table {{.Repository}}\\t{{.Tag}}\\t{{.Size}}\" REPOSITORY TAG SIZE multifractal/template_pandas v3.8.p 817MB nanozoo/rmarkdown 2.10--a3f4088 3.72GB papanikos/marvel 0.2-29b3c73 6.42GB papanikos/virsorter-2 2.2.1--fa935f8 1.19GB multifractal/seeker 0.1 1.66GB multifractal/phigaro 0.5.2 2.6GB nanozoo/seqkit 0.13.2--cd66104 469MB multifractal/virnet-hack 0.1 1.62GB nanozoo/emboss 6.6.0--418c521 1.07GB multifractal/ppr-meta 0.3.1 5.3GB multifractal/virfinder 0.2 3.91GB multifractal/vibrant 0.5 1.42GB nanozoo/sourmash 3.4.1--16a8db7 788MB nanozoo/hmmer 3.3--3db9dd1 484MB nanozoo/checkv 0.6.0--e97f45e 1.72GB nanozoo/altair 4.1.0--086b80e 1.02GB nanozoo/samtools 1.9--76b9270 487MB multifractal/virsorter 0.1.2 2.84GB nanozoo/r_fungi 0.1--097b1bb 3.11GB nanozoo/template 3.8--ccd0653 681MB nanozoo/basics 1.0--962b907 79.1MB nanozoo/upsetr 1.4.0--0ea25b3 3.21GB nanozoo/r_ggplot2 0.1--6405f6d 3.26GB multifractal/metaphinder 0.1 767MB multifractal/deepvirfinder 0.1 2.37GB nanozoo/prodigal 2.6.3--2769024 531MB","title":"Docker-files needed and their size"},{"location":"troubleshooting/#installing-wtp-in-a-centralized-way-for-multi-users","text":"Run WtP on a cluster environment: Let the Users run WtP from their individual accounts (e.g., via ssh connection) create special locations (shared locations) where to store the singularity images, docker images, the databases and the cache let each run of WtP (from a user) use the shared locations but the usage provided forces the users to specify params --workdir, --databases and --cache...... the shared locations should be transparent to the users .. Quick Solution: \"install\" WtP via git clone --branch v1.2.0 https://github.com/replikation/What_the_Phage.git then change the nextflow config and let the user use this \"git\" (the version would then be fixed to the git clone) e.g.: ./phage.nf \\ --fasta /path/to/file.fa \\ # provide a fasta-file as input --cores 8 \\ # number of cores you want to use -profile local,docker # choose the environment:local and docker","title":"Installing WTP in a centralized way for multi-users"},{"location":"troubleshooting/#error-ignored","text":"some tools can fail during the identification process e.g. if a tool can't process a fasta-inputfile (sometimes MARVEL with really large multi fasta files) then you will get an error-message like this : [34/31d18f] NOTE: Process `identify_fasta_MSF:upsetr_plot (1)` terminated with an error exit status (1) -- Error is ignored even if an error occures WtP continues its work, but won't include the results of the failed process","title":"Error ignored:"},{"location":"troubleshooting/#chromomap-issues-terminated-with-an-error-exit-status-1","text":"In the Annotation process of WtP, wird eine Genkarte mit den annotierten/gefundenen Genen erstellt Bei der erstellung der Genkarte kann es manchmal zu folgendem Error-code kommen: [4e/57d82d] NOTE: Process 'phage_annotation_MSF:chromomap (1)' terminated with an error exit status (1) -- Execution is retried (1) The workflow works as intended. We included a few \"failsafe\" processes for chromomap, so if one plotting approach fails it retries another approach to render it, thus you get the fail but the retry will work. In the end you will get the result","title":"Chromomap issues: terminated with an error exit status (1)"},{"location":"troubleshooting/#singularity-image-problems","text":"Sometimes the singularity runs fail because some singularity images are failing in their build process: you can retry the the execution command or you can run the Pre-download for Offline-mode and start WtP with the 'preloaded' images","title":"Singularity image problems"},{"location":"troubleshooting/#download-databases-manually","text":"usually there is no problem with database download. in case there is a problem you can download them here","title":"Download Databases manually"},{"location":"Workflow-execution/Overview/","text":"Command overview The basic command nextflow run \\ # calling the workflow replikation/What_the_Phage \\ # WtP Git-Repo --fasta /path/to/file.fa \\ # provide a fasta-file as input --cores 8 \\ # number of cores you want to use -profile local,docker # choose the environment:local and docker -r v1.2.0 # WtP release version Flag overview Mandatory Flag simple explanation --fasta path/to/phage-assembly.fa or '/path/to/*.fa' -profile local,docker or local,singularity or lsf,docker -r v1.2.0 Options Flag simple explanation --help will show you this page in your terminal --filter e.g. 1500 bp (sequences below 1500 bp won't be analyzed) --cores e.g. 10 --setup pre-downloads all you need to run WtP Pathing Flag simple explanation --workdir /path/to/dir --database /path/to/dir --cachedir /path/to/dir --output e.g. results Workflow control Flag simple explanation --dv deactivates deepvirfinder --mp deactivates metaphinder --pp deactivates PPRmeta --sm deactivates sourmash --vb deactivates vibrant --vf deactivates virfinder --vn deactivates virnet --vs deactivates virsorter --ph deactivates phigaro --vs2 deactivates virsorter2 --identify only phage identification, skips analysis --annotate only annotation, skips phage identification --all_tools activate all included phage prediction tools --annotation_db /path/to/your/custom_phage_annotation_db.tar.gz","title":"Overview"},{"location":"Workflow-execution/Overview/#command-overview","text":"","title":"Command overview"},{"location":"Workflow-execution/Overview/#the-basic-command","text":"nextflow run \\ # calling the workflow replikation/What_the_Phage \\ # WtP Git-Repo --fasta /path/to/file.fa \\ # provide a fasta-file as input --cores 8 \\ # number of cores you want to use -profile local,docker # choose the environment:local and docker -r v1.2.0 # WtP release version","title":"The basic command"},{"location":"Workflow-execution/Overview/#flag-overview","text":"","title":"Flag overview"},{"location":"Workflow-execution/Overview/#mandatory","text":"Flag simple explanation --fasta path/to/phage-assembly.fa or '/path/to/*.fa' -profile local,docker or local,singularity or lsf,docker -r v1.2.0","title":"Mandatory"},{"location":"Workflow-execution/Overview/#options","text":"Flag simple explanation --help will show you this page in your terminal --filter e.g. 1500 bp (sequences below 1500 bp won't be analyzed) --cores e.g. 10 --setup pre-downloads all you need to run WtP","title":"Options"},{"location":"Workflow-execution/Overview/#pathing","text":"Flag simple explanation --workdir /path/to/dir --database /path/to/dir --cachedir /path/to/dir --output e.g. results","title":"Pathing"},{"location":"Workflow-execution/Overview/#workflow-control","text":"Flag simple explanation --dv deactivates deepvirfinder --mp deactivates metaphinder --pp deactivates PPRmeta --sm deactivates sourmash --vb deactivates vibrant --vf deactivates virfinder --vn deactivates virnet --vs deactivates virsorter --ph deactivates phigaro --vs2 deactivates virsorter2 --identify only phage identification, skips analysis --annotate only annotation, skips phage identification --all_tools activate all included phage prediction tools --annotation_db /path/to/your/custom_phage_annotation_db.tar.gz","title":"Workflow control"},{"location":"Workflow-execution/commands/","text":"Detailed Flag explanation Run option by default WtP deploys only a selection of phage prediction tools that was benchmarked by Ho et al. if you wish to run all integrated phage prediction tools: `--all_tools` # activate all included phage prediction tools Inputs Input examples: wildcards need single quotes around the path ( ' ) --fasta /path/to/phage-assembly.fa # path to your fasta-file --fasta '/path/to/*.fa' # path to all .fa files in a dir Profiles Choose the environment: local , slurm , lsf or ebi Choose the engine: docker or singularity examples: -profile local,docker -profile local,singularity -profile lsf,docker Custom phage annotation database --annotation_db will allow you to provide your own database instead of the default pvog database you need the following file types for the annotation process (hmmscan), needed files custom.hmm custom.hmm.h3f custom.hmm.h3i custom.hmm.h3m custom.hmm.h3p tar -czvf custom_db.tar.gz custom.hmm custom.hmm.h3f custom.hmm.h3i custom.hmm.h3m custom.hmm.h3p with --annotation_db custom_db.tar.gz you can provide your own custom phage annotation database Release candidate A release candidate is a released version of WtP which ensures proper functionality version control ensures reproducibility as each tools version is also \"locked\" within the release candidate databases have no automatic version control (they are downloaded from the source) if you need version control for databases, just make a copy of the database dir after download you can specify the database dir via the --database flag (see below) WtP only downloads a database if it's missing, it is not \"auto-updating\" them add this flag to your command and a specific release is used instead -r v1.1.0 Data handling WtP handles everything by default If you need to change paths use the following commands It's useful to specify --workdir to your current working dir if /tmp (default) has limited space -work-dir /path/to/dir # defines the path where nextflow writes temporary files, default: '/tmp/nextflow-phage-$USER' --databases /path/to/dir # specify download location of databases, default './nextflow-autodownload-databases' --cachedir /path/to/dir # defines the path where singularity images are cached, default './singularity-images' --output results # path of the outdir, default './results' Pre-download for Offline-mode --setup skips analysis and just downloads all databases and containers Needs roughly 30 GB storage for databases, excluding programs nextflow run replikation/What_the_Phage --setup -r v1.1.0 -profile singularity,local you can change the database download location via (--databases) make sure that you specify the database location when executing WtP, if you change the default path singularity images sometimes fail during building, just try to re-execute --setup WtP attempts to build images up to 3 times, image building is individually skipped if present","title":"Commands"},{"location":"Workflow-execution/commands/#detailed-flag-explanation","text":"","title":"Detailed Flag explanation"},{"location":"Workflow-execution/commands/#run-option","text":"by default WtP deploys only a selection of phage prediction tools that was benchmarked by Ho et al. if you wish to run all integrated phage prediction tools: `--all_tools` # activate all included phage prediction tools","title":"Run option"},{"location":"Workflow-execution/commands/#inputs","text":"Input examples: wildcards need single quotes around the path ( ' ) --fasta /path/to/phage-assembly.fa # path to your fasta-file --fasta '/path/to/*.fa' # path to all .fa files in a dir","title":"Inputs"},{"location":"Workflow-execution/commands/#profiles","text":"Choose the environment: local , slurm , lsf or ebi Choose the engine: docker or singularity examples: -profile local,docker -profile local,singularity -profile lsf,docker","title":"Profiles"},{"location":"Workflow-execution/commands/#custom-phage-annotation-database","text":"--annotation_db will allow you to provide your own database instead of the default pvog database you need the following file types for the annotation process (hmmscan), needed files custom.hmm custom.hmm.h3f custom.hmm.h3i custom.hmm.h3m custom.hmm.h3p tar -czvf custom_db.tar.gz custom.hmm custom.hmm.h3f custom.hmm.h3i custom.hmm.h3m custom.hmm.h3p with --annotation_db custom_db.tar.gz you can provide your own custom phage annotation database","title":"Custom phage annotation database"},{"location":"Workflow-execution/commands/#release-candidate","text":"A release candidate is a released version of WtP which ensures proper functionality version control ensures reproducibility as each tools version is also \"locked\" within the release candidate databases have no automatic version control (they are downloaded from the source) if you need version control for databases, just make a copy of the database dir after download you can specify the database dir via the --database flag (see below) WtP only downloads a database if it's missing, it is not \"auto-updating\" them add this flag to your command and a specific release is used instead -r v1.1.0","title":"Release candidate"},{"location":"Workflow-execution/commands/#data-handling","text":"WtP handles everything by default If you need to change paths use the following commands It's useful to specify --workdir to your current working dir if /tmp (default) has limited space -work-dir /path/to/dir # defines the path where nextflow writes temporary files, default: '/tmp/nextflow-phage-$USER' --databases /path/to/dir # specify download location of databases, default './nextflow-autodownload-databases' --cachedir /path/to/dir # defines the path where singularity images are cached, default './singularity-images' --output results # path of the outdir, default './results'","title":"Data handling"},{"location":"Workflow-execution/commands/#pre-download-for-offline-mode","text":"--setup skips analysis and just downloads all databases and containers Needs roughly 30 GB storage for databases, excluding programs nextflow run replikation/What_the_Phage --setup -r v1.1.0 -profile singularity,local you can change the database download location via (--databases) make sure that you specify the database location when executing WtP, if you change the default path singularity images sometimes fail during building, just try to re-execute --setup WtP attempts to build images up to 3 times, image building is individually skipped if present","title":"Pre-download for Offline-mode"},{"location":"Workflow-execution/processes/","text":"WtP terminal execution: In this section we want to explain what you see in the Terminal when you execute WtP: I'm currently writing on this part Let's do a Testrun Open your terminal, move to a directory where you want to execute WtP and type/copy the following command nextflow run phage.nf -profile smalltest,local,docker --work-dir work --cores 16 A lot of stuff will pop up but we will quide you through it: N E X T F L O W ~ version 20.07.1 Launching `phage.nf` [admiring_pesquet] - revision: 1b76c726d3 _____ _____ ____ ____ ___ ___ __ __ _ _ __ _______________________ / \\ / \\__ ___/\\______ \\ \\ \\/\\/ / | | | ___/ \\ / | | | | \\__/\\ / |____| |____| \\/ _____ _____ ____ ____ ___ ___ __ __ _ _ Profile: smalltest,local,docker Current User: mike Nextflow-version: 20.07.1 WtP intended for Nextflow-version: 20.01.0 Starting time: 24-07-2020 15:18 UTC Workdir location [--workdir]: /home/mike/bioinformatics/What_the_Phage/cores=16 Output location [--output]: results Database location [--databases]: nextflow-autodownload-databases CPUs to use: 8, maximal CPUs to use: 20 this is the first thing that will pop up and will give you some basic workflow information: eg where output is stored or how many cores you are using for computing An overview about all the WtP processes will show up and looks like this: [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [c7/d34016] process > identify_fasta_MSF:fasta_validation_wf:seqkit (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virsorter_wf:virsorter [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter_wf:filter_virsorter - [- ] process > identify_fasta_MSF:virsorter_wf:virsorter_collect_data - [ff/f698ec] process > identify_fasta_MSF:virsorter2_wf:virsorter2 (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter2_wf:filter_virsorter2 - * * * executor > local (9) [skipped ] process > phage_references:download_references [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > phage_blast_DB:phage_references_blastDB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > ppr_dependecies:ppr_download_dependencies [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vibrant_database:vibrant_download_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > virsorter_database:virsorter_download_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > pvog_database:pvog_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vogtable_database:vogtable_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vog_database:vog_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > rvdb_database:rvdb_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > checkV_database:download_checkV_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > sourmash_database:sourmash_download_DB [100%] 1 of 1, stored: 1 \u2714 [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [c7/d34016] process > identify_fasta_MSF:fasta_validation_wf:seqkit (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virsorter_wf:virsorter [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter_wf:filter_virsorter - [- ] process > identify_fasta_MSF:virsorter_wf:virsorter_collect_data - [ff/f698ec] process > identify_fasta_MSF:virsorter2_wf:virsorter2 (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter2_wf:filter_virsorter2 - [- ] process > identify_fasta_MSF:virsorter2_wf:virsorter2_collect_data - * * * [fc/e0fcc7] process > identify_fasta_MSF:pprmeta_wf:pprmeta (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:pprmeta_wf:filter_PPRmeta - [- ] process > identify_fasta_MSF:pprmeta_wf:pprmeta_collect_data - [- ] process > identify_fasta_MSF:vibrant_wf:vibrant [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:vibrant_wf:filter_vibrant - [- ] process > identify_fasta_MSF:vibrant_wf:vibrant_collect_data - [- ] process > identify_fasta_MSF:vibrant_virome_wf:vibrant_virome [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:vibrant_virome_wf:filter_vibrant_virome - [- ] process > identify_fasta_MSF:vibrant_virome_wf:vibrant_virome_collect_data - [08/50ce71] process > identify_fasta_MSF:virnet_wf:normalize_contig_size (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virnet_wf:virnet [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virnet_wf:filter_virnet - [- ] process > identify_fasta_MSF:virnet_wf:virnet_collect_data - [4e/ca5a07] process > identify_fasta_MSF:phigaro_wf:phigaro (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:phigaro_wf:phigaro_collect_data - [- ] process > identify_fasta_MSF:seeker_wf:seeker [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:seeker_wf:filter_seeker - [- ] process > identify_fasta_MSF:seeker_wf:seeker_collect_data - [- ] process > identify_fasta_MSF:filter_tool_names - [- ] process > identify_fasta_MSF:r_plot - [- ] process > identify_fasta_MSF:upsetr_plot - [- ] process > identify_fasta_MSF:samtools - [- ] process > phage_annotation_MSF:prodigal - [- ] process > phage_annotation_MSF:hmmscan - [- ] process > phage_annotation_MSF:chromomap_parser - [- ] process > phage_annotation_MSF:chromomap - [- ] process > checkV_wf:checkV - [- ] process > phage_tax_classification:split_multi_fasta - [- ] process > phage_tax_classification:sourmash_for_tax - Started process [4e/ca5a07] process > identify_fasta_MSF:phigaro_wf:phigaro (1) [ 0%] 0 of 1 [4e/ca5a07] : is the process folder where the tasks of this process is started/executed identify_fasta_MSF : is one of two sub workflows (identify sequence; phage annotation) phigaro_wf: : the Phigaro identification process contains a smaller workflow with phigaro and phigaro_collect_data (raw output of Phigaro) phigaro : is the actual phage identification task [ 0%] 0 of 1 : Process has not finished yet Finished Process this is how a finished process will look like this: [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [6b/79564a] : is the process folder where the tasks of this process (input_suffix_check) is executed you can access the folder: default: cd /tmp/nextflow-phage-$USER/6b/79564a and tabcompletion in our case with --work-dir : cd work/6b/79564a and tab completion you will see the files used/created in this process identify_fasta_MSF : is one of two sub workflows (identify sequence; phage annotation) input_suffix_check (1) : is the actual task (in this case: checks if the fasta file has the correct format) [100%] 1 of 1 \u2714 : tells you that the process is complete Stored process [skipped ] process > phage_blast_DB:phage_references_blastDB [100%] 1 of 1, stored: 1 \u2714 this is shown when you already e.g. downloaded the database so WtP won't execute this process again because the data is already stored Process in Queue [- ] process > identify_fasta_MSF:phigaro_wf:phigaro_collect_data - this is shows you that the process has not been started yet and is queued and waiting for execution OnComplete Done! Results are stored here --> results Thank you for using What the Phage Please cite us: https://doi.org/10.1101/2020.07.24.219899 Please also cite the other tools we use in our workflow --> results/literature Completed at: 30-Oct-2020 10:15:27 Duration : 49m 34s CPU hours : 5.2 Succeeded : 61 the onComplete message is shown when WtP is done here you find again some basic information e.g. how long was your run, where are the results and a literature-file for citing us and the tools WtP uses","title":"Testrun"},{"location":"Workflow-execution/processes/#wtp-terminal-execution","text":"In this section we want to explain what you see in the Terminal when you execute WtP: I'm currently writing on this part","title":"WtP terminal execution:"},{"location":"Workflow-execution/processes/#lets-do-a-testrun","text":"Open your terminal, move to a directory where you want to execute WtP and type/copy the following command nextflow run phage.nf -profile smalltest,local,docker --work-dir work --cores 16 A lot of stuff will pop up but we will quide you through it: N E X T F L O W ~ version 20.07.1 Launching `phage.nf` [admiring_pesquet] - revision: 1b76c726d3 _____ _____ ____ ____ ___ ___ __ __ _ _ __ _______________________ / \\ / \\__ ___/\\______ \\ \\ \\/\\/ / | | | ___/ \\ / | | | | \\__/\\ / |____| |____| \\/ _____ _____ ____ ____ ___ ___ __ __ _ _ Profile: smalltest,local,docker Current User: mike Nextflow-version: 20.07.1 WtP intended for Nextflow-version: 20.01.0 Starting time: 24-07-2020 15:18 UTC Workdir location [--workdir]: /home/mike/bioinformatics/What_the_Phage/cores=16 Output location [--output]: results Database location [--databases]: nextflow-autodownload-databases CPUs to use: 8, maximal CPUs to use: 20 this is the first thing that will pop up and will give you some basic workflow information: eg where output is stored or how many cores you are using for computing An overview about all the WtP processes will show up and looks like this: [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [c7/d34016] process > identify_fasta_MSF:fasta_validation_wf:seqkit (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virsorter_wf:virsorter [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter_wf:filter_virsorter - [- ] process > identify_fasta_MSF:virsorter_wf:virsorter_collect_data - [ff/f698ec] process > identify_fasta_MSF:virsorter2_wf:virsorter2 (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter2_wf:filter_virsorter2 - * * * executor > local (9) [skipped ] process > phage_references:download_references [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > phage_blast_DB:phage_references_blastDB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > ppr_dependecies:ppr_download_dependencies [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vibrant_database:vibrant_download_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > virsorter_database:virsorter_download_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > pvog_database:pvog_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vogtable_database:vogtable_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > vog_database:vog_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > rvdb_database:rvdb_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > checkV_database:download_checkV_DB [100%] 1 of 1, stored: 1 \u2714 [skipped ] process > sourmash_database:sourmash_download_DB [100%] 1 of 1, stored: 1 \u2714 [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [c7/d34016] process > identify_fasta_MSF:fasta_validation_wf:seqkit (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virsorter_wf:virsorter [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter_wf:filter_virsorter - [- ] process > identify_fasta_MSF:virsorter_wf:virsorter_collect_data - [ff/f698ec] process > identify_fasta_MSF:virsorter2_wf:virsorter2 (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virsorter2_wf:filter_virsorter2 - [- ] process > identify_fasta_MSF:virsorter2_wf:virsorter2_collect_data - * * * [fc/e0fcc7] process > identify_fasta_MSF:pprmeta_wf:pprmeta (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:pprmeta_wf:filter_PPRmeta - [- ] process > identify_fasta_MSF:pprmeta_wf:pprmeta_collect_data - [- ] process > identify_fasta_MSF:vibrant_wf:vibrant [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:vibrant_wf:filter_vibrant - [- ] process > identify_fasta_MSF:vibrant_wf:vibrant_collect_data - [- ] process > identify_fasta_MSF:vibrant_virome_wf:vibrant_virome [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:vibrant_virome_wf:filter_vibrant_virome - [- ] process > identify_fasta_MSF:vibrant_virome_wf:vibrant_virome_collect_data - [08/50ce71] process > identify_fasta_MSF:virnet_wf:normalize_contig_size (1) [100%] 1 of 1 \u2714 [- ] process > identify_fasta_MSF:virnet_wf:virnet [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:virnet_wf:filter_virnet - [- ] process > identify_fasta_MSF:virnet_wf:virnet_collect_data - [4e/ca5a07] process > identify_fasta_MSF:phigaro_wf:phigaro (1) [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:phigaro_wf:phigaro_collect_data - [- ] process > identify_fasta_MSF:seeker_wf:seeker [ 0%] 0 of 1 [- ] process > identify_fasta_MSF:seeker_wf:filter_seeker - [- ] process > identify_fasta_MSF:seeker_wf:seeker_collect_data - [- ] process > identify_fasta_MSF:filter_tool_names - [- ] process > identify_fasta_MSF:r_plot - [- ] process > identify_fasta_MSF:upsetr_plot - [- ] process > identify_fasta_MSF:samtools - [- ] process > phage_annotation_MSF:prodigal - [- ] process > phage_annotation_MSF:hmmscan - [- ] process > phage_annotation_MSF:chromomap_parser - [- ] process > phage_annotation_MSF:chromomap - [- ] process > checkV_wf:checkV - [- ] process > phage_tax_classification:split_multi_fasta - [- ] process > phage_tax_classification:sourmash_for_tax -","title":"Let's do a Testrun"},{"location":"Workflow-execution/processes/#started-process","text":"[4e/ca5a07] process > identify_fasta_MSF:phigaro_wf:phigaro (1) [ 0%] 0 of 1 [4e/ca5a07] : is the process folder where the tasks of this process is started/executed identify_fasta_MSF : is one of two sub workflows (identify sequence; phage annotation) phigaro_wf: : the Phigaro identification process contains a smaller workflow with phigaro and phigaro_collect_data (raw output of Phigaro) phigaro : is the actual phage identification task [ 0%] 0 of 1 : Process has not finished yet","title":"Started process"},{"location":"Workflow-execution/processes/#finished-process","text":"this is how a finished process will look like this: [6b/79564a] process > identify_fasta_MSF:fasta_validation_wf:input_suffix_check (1) [100%] 1 of 1 \u2714 [6b/79564a] : is the process folder where the tasks of this process (input_suffix_check) is executed you can access the folder: default: cd /tmp/nextflow-phage-$USER/6b/79564a and tabcompletion in our case with --work-dir : cd work/6b/79564a and tab completion you will see the files used/created in this process identify_fasta_MSF : is one of two sub workflows (identify sequence; phage annotation) input_suffix_check (1) : is the actual task (in this case: checks if the fasta file has the correct format) [100%] 1 of 1 \u2714 : tells you that the process is complete","title":"Finished Process"},{"location":"Workflow-execution/processes/#stored-process","text":"[skipped ] process > phage_blast_DB:phage_references_blastDB [100%] 1 of 1, stored: 1 \u2714 this is shown when you already e.g. downloaded the database so WtP won't execute this process again because the data is already stored","title":"Stored process"},{"location":"Workflow-execution/processes/#process-in-queue","text":"[- ] process > identify_fasta_MSF:phigaro_wf:phigaro_collect_data - this is shows you that the process has not been started yet and is queued and waiting for execution","title":"Process in Queue"},{"location":"Workflow-execution/processes/#oncomplete","text":"Done! Results are stored here --> results Thank you for using What the Phage Please cite us: https://doi.org/10.1101/2020.07.24.219899 Please also cite the other tools we use in our workflow --> results/literature Completed at: 30-Oct-2020 10:15:27 Duration : 49m 34s CPU hours : 5.2 Succeeded : 61 the onComplete message is shown when WtP is done here you find again some basic information e.g. how long was your run, where are the results and a literature-file for citing us and the tools WtP uses","title":"OnComplete"},{"location":"Workflow-execution/results/","text":"Example run We executed the following commands do perform a WtP test-run from the Terminal-section: nextflow run phage.nf -profile smalltest,local,docker --work-dir --cores 16 WtP will create a results -folder in your current working dir (where you executed WtP) and a subfolder with the name of your input-fasta - in our case all_pos_phage Results literature contains a Citations.bib file which you can import the in your citation program to have all the citations of the programs WtP uses for its analysis run info execution_report.html: this report gives an overview over: distribution of resource usage for each process (CPU, Memory, Job duration) information about each task in the workflow execution_timeline.html: this report gives you an overview over the processes execution timeline Test sample result A testreport for a larger dataset can be found here raw_data contains the raw output from the phage-identification-tools packed as toolname.tar.gz -file sample_overview-small.html, sample_overview-large.html The graphical output of the annotation shows an overview of the individual loci of the predicted ORFs and the corresponding genes in the fasta sequences identified as phages. For a better visibility, we have chosen 4 categories: tail, capsid, baseplate, and other. This output can be used to verify the identified sequences (if the predicted sequences make sense or not). The annotation results are additionally plotted in an interactive HTML-file and are available as a file for further analysis.","title":"Results"},{"location":"Workflow-execution/results/#example-run","text":"We executed the following commands do perform a WtP test-run from the Terminal-section: nextflow run phage.nf -profile smalltest,local,docker --work-dir --cores 16 WtP will create a results -folder in your current working dir (where you executed WtP) and a subfolder with the name of your input-fasta - in our case all_pos_phage","title":"Example run"},{"location":"Workflow-execution/results/#results","text":"literature contains a Citations.bib file which you can import the in your citation program to have all the citations of the programs WtP uses for its analysis run info execution_report.html: this report gives an overview over: distribution of resource usage for each process (CPU, Memory, Job duration) information about each task in the workflow execution_timeline.html: this report gives you an overview over the processes execution timeline","title":"Results"},{"location":"Workflow-execution/results/#test-sample-result","text":"A testreport for a larger dataset can be found here raw_data contains the raw output from the phage-identification-tools packed as toolname.tar.gz -file sample_overview-small.html, sample_overview-large.html The graphical output of the annotation shows an overview of the individual loci of the predicted ORFs and the corresponding genes in the fasta sequences identified as phages. For a better visibility, we have chosen 4 categories: tail, capsid, baseplate, and other. This output can be used to verify the identified sequences (if the predicted sequences make sense or not). The annotation results are additionally plotted in an interactive HTML-file and are available as a file for further analysis.","title":"Test sample result"},{"location":"installation/nextflow/","text":"Install Java runtime and Nextflow Nextflow is quite easy to install: install a java run time for Nextflow if not available sudo apt-get update sudo apt install -y default-jre install nextflow : # download curl -s https://get.nextflow.io | bash # move to any $PATH location or add it to $PATH # e.g. sudo mv nextflow /usr/bin/ Update Nextflow to a newer version just execute this: alternatively use the \"build in\" nextflow upgrade # downloads the newer version curl -s https://get.nextflow.io | bash # replace the old version with the newer version sudo mv nextflow /usr/bin/ Specific Nextflow versions if you need a specific older nextflow version for some reason, go to the release page under assets you find a file called \"nextflow-RELEASENUMBER-all\" (replace RELEASENUMBER with the actual number they provide on the release page) download this file and move it to e.g. /usr/bin/ or call it directly ./nextflow-RELEASENUMBER-all run ... instead of nextflow run ... you do nextflow-RELEASENUMBER-all run ...","title":"Nextflow"},{"location":"installation/nextflow/#install-java-runtime-and-nextflow","text":"Nextflow is quite easy to install: install a java run time for Nextflow if not available sudo apt-get update sudo apt install -y default-jre install nextflow : # download curl -s https://get.nextflow.io | bash # move to any $PATH location or add it to $PATH # e.g. sudo mv nextflow /usr/bin/","title":"Install Java runtime and Nextflow"},{"location":"installation/nextflow/#update-nextflow-to-a-newer-version","text":"just execute this: alternatively use the \"build in\" nextflow upgrade # downloads the newer version curl -s https://get.nextflow.io | bash # replace the old version with the newer version sudo mv nextflow /usr/bin/","title":"Update Nextflow to a newer version"},{"location":"installation/nextflow/#specific-nextflow-versions","text":"if you need a specific older nextflow version for some reason, go to the release page under assets you find a file called \"nextflow-RELEASENUMBER-all\" (replace RELEASENUMBER with the actual number they provide on the release page) download this file and move it to e.g. /usr/bin/ or call it directly ./nextflow-RELEASENUMBER-all run ... instead of nextflow run ... you do nextflow-RELEASENUMBER-all run ...","title":"Specific Nextflow versions"},{"location":"installation/quick_installation/","text":"Local, docker usage \"None informaticians / newcomer to bioinformatics\" approach using ubuntu [admin rights required] we do not recommend this installation-process (an older version of Docker will be installed. This installation method is not suitable for a cluster), but it is probably the fastest way to get WtP to run sudo apt-get update sudo apt install -y default-jre curl -s https://get.nextflow.io | bash sudo mv nextflow /usr/bin/ sudo apt-get install -y docker-ce docker-ce-cli containerd.io sudo usermod -a -G docker $USER Restart your computer Conda usage: conda create -n wtp nextflow==20.07.01 singularity==3.6.1 conda activate wtp (wtp) $ nextflow run replikation/What_the_Phage -r v1.2.0 --setup ... now you should be able to run nextflow run replikation/What_the_Phage -r v1.2.0 --setup -profile local,singularity Test the workflow latest workflow release nextflow pull replikation/What_the_Phage for docker (local use) nextflow run replikation/What_the_Phage -r v1.2.0 --cores 8 -profile smalltest,local,docker for singularity (slurm use) nextflow run replikation/What_the_Phage -r v1.2.0 --cores 8 -profile smalltest,slurm,singularity","title":"Quick Installation"},{"location":"installation/quick_installation/#local-docker-usage","text":"\"None informaticians / newcomer to bioinformatics\" approach using ubuntu [admin rights required] we do not recommend this installation-process (an older version of Docker will be installed. This installation method is not suitable for a cluster), but it is probably the fastest way to get WtP to run sudo apt-get update sudo apt install -y default-jre curl -s https://get.nextflow.io | bash sudo mv nextflow /usr/bin/ sudo apt-get install -y docker-ce docker-ce-cli containerd.io sudo usermod -a -G docker $USER Restart your computer","title":"Local, docker usage"},{"location":"installation/quick_installation/#conda-usage","text":"conda create -n wtp nextflow==20.07.01 singularity==3.6.1 conda activate wtp (wtp) $ nextflow run replikation/What_the_Phage -r v1.2.0 --setup ... now you should be able to run nextflow run replikation/What_the_Phage -r v1.2.0 --setup -profile local,singularity","title":"Conda usage:"},{"location":"installation/quick_installation/#test-the-workflow","text":"latest workflow release nextflow pull replikation/What_the_Phage for docker (local use) nextflow run replikation/What_the_Phage -r v1.2.0 --cores 8 -profile smalltest,local,docker for singularity (slurm use) nextflow run replikation/What_the_Phage -r v1.2.0 --cores 8 -profile smalltest,slurm,singularity","title":"Test the workflow"},{"location":"installation/Engine/docker/","text":"Docker installation to install the most recent version we recommend the following Docker installation they also describe how to install docker on debian, or other distros in the following section we describe how to install docker on ubuntu For Ubuntu we are using this installation guide but shorten it to the bare minimum for you so we skip a few steps that are not necessary Setting up the repository Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository (via apt update and apt upgrade ). sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common Add Docker\u2019s official GPG key: we skip the verification here curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Note: The lsb_release -cs sub-command below returns the name of your Ubuntu distribution, such as xenial. Sometimes, in a distribution like Linux Mint, you might need to change $(lsb_release -cs) to your parent Ubuntu distribution. For example, if you are using Linux Mint Tessa, you could use bionic. Docker does not offer any guarantees on untested and unsupported Ubuntu distributions. for x86_64 / amd64 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\" Install Docker Engine install via sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io Verify that Docker Engine is installed correctly by running the hello-world image. sudo docker run --rm hello-world This command downloads a test image and runs it in a container. When the container runs, it prints an informational message and exits. Docker Engine is installed and running. Create docker group docker needs to run without sudo for nextflow so we need to add user that want to use docker to the docker group Create the docker group (should be already created after the installation) sudo groupadd docker add a user to the docker group. sudo usermod -aG docker $USER Restart","title":"Docker"},{"location":"installation/Engine/docker/#docker-installation","text":"to install the most recent version we recommend the following Docker installation they also describe how to install docker on debian, or other distros in the following section we describe how to install docker on ubuntu","title":"Docker installation"},{"location":"installation/Engine/docker/#for-ubuntu","text":"we are using this installation guide but shorten it to the bare minimum for you so we skip a few steps that are not necessary","title":"For Ubuntu"},{"location":"installation/Engine/docker/#setting-up-the-repository","text":"Before you install Docker Engine for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository (via apt update and apt upgrade ). sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg-agent \\ software-properties-common Add Docker\u2019s official GPG key: we skip the verification here curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Note: The lsb_release -cs sub-command below returns the name of your Ubuntu distribution, such as xenial. Sometimes, in a distribution like Linux Mint, you might need to change $(lsb_release -cs) to your parent Ubuntu distribution. For example, if you are using Linux Mint Tessa, you could use bionic. Docker does not offer any guarantees on untested and unsupported Ubuntu distributions. for x86_64 / amd64 sudo add-apt-repository \\ \"deb [arch=amd64] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) \\ stable\"","title":"Setting up the repository"},{"location":"installation/Engine/docker/#install-docker-engine","text":"install via sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io Verify that Docker Engine is installed correctly by running the hello-world image. sudo docker run --rm hello-world This command downloads a test image and runs it in a container. When the container runs, it prints an informational message and exits. Docker Engine is installed and running.","title":"Install Docker Engine"},{"location":"installation/Engine/docker/#create-docker-group","text":"docker needs to run without sudo for nextflow so we need to add user that want to use docker to the docker group Create the docker group (should be already created after the installation) sudo groupadd docker add a user to the docker group. sudo usermod -aG docker $USER Restart","title":"Create docker group"},{"location":"installation/Engine/singularity/","text":"Singularity installation we are using this installation guide For Ubuntu Install system dependencies You must first install development tools and libraries to your host. $ sudo apt-get update && \\ sudo apt-get install -y build-essential \\ libseccomp-dev pkg-config squashfs-tools cryptsetup Install Golang This is one of several ways to install and configure golang. First, download the Golang archive to /tmp, then extract the archive to /usr/local. NOTE: if you are updating Go from an older version, make sure you remove /usr/local/go before reinstalling it. export VERSION=1.14.9 OS=linux ARCH=amd64 # change this as you need wget -O /tmp/go${VERSION}.${OS}-${ARCH}.tar.gz https://dl.google.com/go/go${VERSION}.${OS}-${ARCH}.tar.gz && \\ sudo tar -C /usr/local -xzf /tmp/go${VERSION}.${OS}-${ARCH}.tar.gz Finally, set up your environment for Go: echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \\ echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \\ source ~/.bashrc Install golangci-lint In order to install golangci-lint, you can run: curl -sfL https://install.goreleaser.com/github.com/golangci/golangci-lint.sh | sh -s -- -b $(go env GOPATH)/bin v1.21.0 This will download and install golangci-lint from its Github releases page (using version v1.21.0 at the moment). Clone the repo Golang is a bit finicky about where things are placed. Here is the correct way to build Singularity from source: mkdir -p ${GOPATH}/src/github.com/sylabs && \\ cd ${GOPATH}/src/github.com/sylabs && \\ git clone https://github.com/sylabs/singularity.git && \\ cd singularity To build a stable version of Singularity, check out a release tag before compiling: git checkout v3.6.3 Compiling Singularity You can build Singularity using the following commands: cd ${GOPATH}/src/github.com/sylabs/singularity && \\ ./mconfig && \\ cd ./builddir && \\ make && \\ sudo make install And that's it! Now you can check your Singularity version by running: singularity version To build in a different folder and to set the install prefix to a different path: ./mconfig -b ./buildtree -p /usr/local","title":"Singularity"},{"location":"installation/Engine/singularity/#singularity-installation","text":"we are using this installation guide","title":"Singularity installation"},{"location":"installation/Engine/singularity/#for-ubuntu","text":"Install system dependencies You must first install development tools and libraries to your host. $ sudo apt-get update && \\ sudo apt-get install -y build-essential \\ libseccomp-dev pkg-config squashfs-tools cryptsetup","title":"For Ubuntu"},{"location":"installation/Engine/singularity/#install-golang","text":"This is one of several ways to install and configure golang. First, download the Golang archive to /tmp, then extract the archive to /usr/local. NOTE: if you are updating Go from an older version, make sure you remove /usr/local/go before reinstalling it. export VERSION=1.14.9 OS=linux ARCH=amd64 # change this as you need wget -O /tmp/go${VERSION}.${OS}-${ARCH}.tar.gz https://dl.google.com/go/go${VERSION}.${OS}-${ARCH}.tar.gz && \\ sudo tar -C /usr/local -xzf /tmp/go${VERSION}.${OS}-${ARCH}.tar.gz Finally, set up your environment for Go: echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \\ echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \\ source ~/.bashrc","title":"Install Golang"},{"location":"installation/Engine/singularity/#install-golangci-lint","text":"In order to install golangci-lint, you can run: curl -sfL https://install.goreleaser.com/github.com/golangci/golangci-lint.sh | sh -s -- -b $(go env GOPATH)/bin v1.21.0 This will download and install golangci-lint from its Github releases page (using version v1.21.0 at the moment).","title":"Install golangci-lint"},{"location":"installation/Engine/singularity/#clone-the-repo","text":"Golang is a bit finicky about where things are placed. Here is the correct way to build Singularity from source: mkdir -p ${GOPATH}/src/github.com/sylabs && \\ cd ${GOPATH}/src/github.com/sylabs && \\ git clone https://github.com/sylabs/singularity.git && \\ cd singularity To build a stable version of Singularity, check out a release tag before compiling: git checkout v3.6.3","title":"Clone the repo"},{"location":"installation/Engine/singularity/#compiling-singularity","text":"You can build Singularity using the following commands: cd ${GOPATH}/src/github.com/sylabs/singularity && \\ ./mconfig && \\ cd ./builddir && \\ make && \\ sudo make install And that's it! Now you can check your Singularity version by running: singularity version To build in a different folder and to set the install prefix to a different path: ./mconfig -b ./buildtree -p /usr/local","title":"Compiling Singularity"},{"location":"installation/Environment/environment/","text":"WtP can be executed in different environments (workload management platforms) Configurations Work in progress coming soon","title":"Local/Cluster"},{"location":"installation/Environment/environment/#configurations","text":"Work in progress coming soon","title":"Configurations"}]}